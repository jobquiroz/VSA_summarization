{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function *Summarize_texts* is called in this notebook.\n",
    "\n",
    "This function contains the following input parameters: \n",
    "\n",
    "- `max_d_list`: A list of values for the hyperparameter *max_d* (between [0, 1])\n",
    "- `diff_bits_list`: List of values for the *diff_bits* hyperparameter (between [0,5000])\n",
    "- `max_d_sent_list`: List of values for the  *max_d_sent* hyperparameter (between [0, 5000])\n",
    "- `size_summary`: Summary size, by default 200.\n",
    "- `texts_list`: Number of documents to summarize from 1 to 57 (all)\n",
    "- `Directory`: Location of the files to summarize\n",
    "- `Precalc_Directory`: Location to store precalc distances used to save time\n",
    "- `output_dir`: Final location of summary files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling summarization function\n",
    "\n",
    "An example of how Summarize_Texts can be called to summarize all texts in a given directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d082a.txt\n",
      "d083a.txt\n",
      "d084a.txt\n",
      "d085d.txt\n",
      "d086d.txt\n",
      "d087d.txt\n",
      "d089d.txt\n",
      "d090d.txt\n",
      "d091c.txt\n",
      "d092c.txt\n",
      "d093c.txt\n",
      "d094c.txt\n",
      "d095c.txt\n",
      "d096c.txt\n",
      "d097e.txt\n",
      "d099e.txt\n",
      "d100e.txt\n",
      "d101e.txt\n",
      "d102e.txt\n",
      "d103g.txt\n",
      "d104g.txt\n",
      "d105g.txt\n",
      "d106g.txt\n",
      "d107g.txt\n",
      "d108g.txt\n",
      "d109h.txt\n",
      "d110h.txt\n",
      "d111h.txt\n",
      "d112h.txt\n",
      "d113h.txt\n",
      "d114h.txt\n",
      "d115i.txt\n",
      "d116i.txt\n",
      "d117i.txt\n",
      "d118i.txt\n",
      "d119i.txt\n",
      "d120i.txt\n"
     ]
    }
   ],
   "source": [
    "%run Summarize_Texts.ipynb\n",
    "\n",
    "Summarize_texts(max_d_list = [0.76],        # Between [0,1] \n",
    "              diff_bits_list = [2000],      # Between [0, 5000] \n",
    "              max_d_sent_list = [4000],     # Between [0, 5000]\n",
    "              size_summary = 200,           # Summari size [100, 200, 400])\n",
    "               Directory = 'Texts/',        # Text directory\n",
    "               output_dir = 'Summaries/')   # Output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "The following functions require to have installed pyrouge, a wrapper for ROUGE155"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python wrapper for the ROUGE summarization evaluation package\n",
    "from pyrouge import Rouge155\n",
    "\n",
    "# Miscellaneous operating system interfaces\n",
    "import os\n",
    "\n",
    "# High-level file operations\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rouge_one_file(rg_path, ps, pm, name_summmary, name_goold_std, year, folder_results, folder_experiment, ID_names_summaries, ID_names_gold_std, gen_summaries, ID_gold_std):\n",
    "    # variables to evaluate summaries\n",
    "    r = Rouge155()\n",
    "    \n",
    "    # summaries path\n",
    "    r.system_dir = rg_path + ps\n",
    "\n",
    "    # gold standard documents path\n",
    "    r.model_dir = rg_path + pm\n",
    "    \n",
    "    r.system_filename_pattern = '({}{}).txt'.format(ID_names_summaries, name_summmary)\n",
    "    r.model_filename_pattern = '({}{}).txt'.format(ID_names_gold_std, name_goold_std)\n",
    "\n",
    "    output = r.convert_and_evaluate()\n",
    "    output_dict = r.output_to_dict(output)\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prom_rouge_golds(n_gs, rouge_meas_list):\n",
    "    \n",
    "    #prom_rouge = []\n",
    "    s = int(len(rouge_meas_list)/n_gs)\n",
    "    \n",
    "    for i in range(s):\n",
    "        prom_rouge = (rouge_meas_list[i] + rouge_meas_list[s + i])/n_gs\n",
    "\n",
    "    return prom_rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROUGE_measures_summaries(ID_gold_std, #folder_gen_summaries, \n",
    "                            ID_names_summaries, \n",
    "                            ID_names_gold_std, name_goold_std, \n",
    "                            sc_gold_std_path, name_summmary, \n",
    "                            rg_gold_std_path, sc_summaries_path, \n",
    "                            rg_summaries_path, rg_path, ps, pm, \n",
    "                            year, folder_results, folder_experiment \n",
    "                            #gen_summaries\n",
    "                            ):\n",
    "\n",
    "    r1r = []\n",
    "    r1p = []\n",
    "    r1f = []\n",
    "    r2r = []\n",
    "    r2p = []\n",
    "    r2f = []\n",
    "    avr_r1r_list = []\n",
    "    avr_r1p_list = []\n",
    "    avr_r1f_list = []\n",
    "    avr_r2r_list = []\n",
    "    avr_r2p_list = []\n",
    "    avr_r2f_list = []\n",
    "    avr_r1r_all_golds_list = []\n",
    "    avr_r1p_all_golds_list = []\n",
    "    avr_r1f_all_golds_list = []\n",
    "    avr_r2r_all_golds_list = []\n",
    "    avr_r2p_all_golds_list = []\n",
    "    avr_r2f_all_golds_list = []\n",
    "\n",
    "    # Iterates gold standards documents\n",
    "    for id_gold in ID_gold_std:\n",
    "\n",
    "        # principal path for gold standard documents\n",
    "        pr_gold_std_path = '/Users/flintlock/Desktop/model_summaries{}_2002_unsup'.format(id_gold)\n",
    "\n",
    "\n",
    "\n",
    "        folder_gen_summaries, gen_summaries = folders_2_list(current_dir, folder_experiment)\n",
    "        \n",
    "        \n",
    "        #Iterates summaries in each folder\n",
    "        for i in range(len(ID_names_summaries)):\n",
    "            \n",
    "            # Reading generated summaries and gold standards documents\n",
    "            shutil.copy('{}/{}{}.txt'.format(pr_gold_std_path, ID_names_gold_std[i], name_goold_std), '{}/{}{}.txt'.format(sc_gold_std_path, ID_names_gold_std[i], name_goold_std))\n",
    "            shutil.copy('{}/{}{}.txt'.format(folder_gen_summaries, ID_names_summaries[i], name_summmary), '{}/{}{}.txt'.format(sc_summaries_path, ID_names_summaries[i], name_summmary))        \n",
    "                \n",
    "            # Moving both files to ROUGE path\n",
    "            shutil.move('{}/{}{}.txt'.format(sc_gold_std_path, ID_names_gold_std[i], name_goold_std), '{}/{}{}.txt'.format(rg_gold_std_path, ID_names_gold_std[i], name_goold_std))\n",
    "            shutil.move('{}/{}{}.txt'.format(sc_summaries_path, ID_names_summaries[i], name_summmary), '{}/{}{}.txt'.format(rg_summaries_path, ID_names_summaries[i], name_summmary))        \n",
    "\n",
    "            # ROUGE measures\n",
    "            output_dict = evaluate_rouge_one_file(rg_path, ps, pm, name_summmary, name_goold_std, year, folder_results, folder_experiment, ID_names_summaries[i], ID_names_gold_std[i], gen_summaries, id_gold)\n",
    "\n",
    "            # Obtaining ROUGE measures\n",
    "            r1r.append(output_dict['rouge_1_recall'])\n",
    "            r1p.append(output_dict['rouge_1_precision'])\n",
    "            r1f.append(output_dict['rouge_1_f_score'])\n",
    "            r2r.append(output_dict['rouge_2_recall'])\n",
    "            r2p.append(output_dict['rouge_2_precision'])\n",
    "            r2f.append(output_dict['rouge_2_f_score'])\n",
    "\n",
    "            # Deleting both files from ROUGE path\n",
    "            os.remove('{}/{}{}.txt'.format(rg_summaries_path, ID_names_summaries[i], name_summmary))\n",
    "            os.remove('{}/{}{}.txt'.format(rg_gold_std_path, ID_names_gold_std[i], name_goold_std))\n",
    "\n",
    "        # Averaging ROUGE measures\n",
    "        avr_r1r = sum(r1r)/len(r1r)\n",
    "        avr_r1p = sum(r1p)/len(r1p)\n",
    "        avr_r1f = sum(r1f)/len(r1f)\n",
    "\n",
    "        avr_r2r = sum(r2r)/len(r2r)\n",
    "        avr_r2p = sum(r2p)/len(r2p)\n",
    "        avr_r2f = sum(r2f)/len(r2f)\n",
    "\n",
    "        avr = [avr_r1r, avr_r1p, avr_r1f, avr_r2r, avr_r2p, avr_r2f]\n",
    "\n",
    "        avr_r1r_list.append(avr_r1r)\n",
    "        avr_r1p_list.append(avr_r1p)\n",
    "        avr_r1f_list.append(avr_r1f)\n",
    "\n",
    "        avr_r2r_list.append(avr_r2r)\n",
    "        avr_r2p_list.append(avr_r2p)\n",
    "        avr_r2f_list.append(avr_r2f)\n",
    "\n",
    "        \n",
    "        \n",
    "        r1r = []\n",
    "        r1p = []\n",
    "        r1f = []\n",
    "        r2r = []\n",
    "        r2p = []\n",
    "        r2f = []\n",
    "        output_dict = 0\n",
    "\n",
    "    # change i for k when the cycle is 1\n",
    "    avr_r1r_all_golds = prom_rouge_golds(len(ID_gold_std), avr_r1r_list)\n",
    "    avr_r1p_all_golds = prom_rouge_golds(len(ID_gold_std), avr_r1p_list)\n",
    "    avr_r1f_all_golds = prom_rouge_golds(len(ID_gold_std), avr_r1f_list)\n",
    "\n",
    "    avr_r2r_all_golds = prom_rouge_golds(len(ID_gold_std), avr_r2r_list)\n",
    "    avr_r2p_all_golds = prom_rouge_golds(len(ID_gold_std), avr_r2p_list)\n",
    "    avr_r2f_all_golds = prom_rouge_golds(len(ID_gold_std), avr_r2f_list)\n",
    "\n",
    "    \n",
    "    avr_all_golds = [avr_r1r_all_golds, avr_r1p_all_golds, avr_r1f_all_golds, avr_r2r_all_golds, avr_r2p_all_golds, avr_r2f_all_golds]\n",
    "\n",
    "    # ROUGE measures of experiments\n",
    "    avr_r1r_all_golds_list.append(avr_r1r_all_golds)\n",
    "    avr_r1p_all_golds_list.append(avr_r1p_all_golds)\n",
    "    avr_r1f_all_golds_list.append(avr_r1f_all_golds)\n",
    "\n",
    "    avr_r2r_all_golds_list.append(avr_r2r_all_golds)\n",
    "    avr_r2p_all_golds_list.append(avr_r2p_all_golds)\n",
    "    avr_r2f_all_golds_list.append(avr_r2f_all_golds)\n",
    "\n",
    "    return [avr_r1r_all_golds, avr_r2r_all_golds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating with ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# returns current working directory of a process\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# folder name of experiment\n",
    "folder_experiment = '/Summaries'\n",
    "\n",
    "# ROUGE needs to know where your summaries and the gold standard documents are\n",
    "rg_path = '/Users/flintlock/pyrouge/build/lib/pyrouge/tests/data'\n",
    "\n",
    "# summaries path\n",
    "ps = '/system_summaries_2004_labelling'\n",
    "\n",
    "# gold standard documents path\n",
    "pm = '/model_summariesA_2004_sup'\n",
    "\n",
    "# folder name in which ROUGE results are saved\n",
    "folder_results = current_dir + '/experiments'\n",
    "\n",
    "# ID names of multi-document DUC 2002\n",
    "ID_names_summaries2 = np.load('folder_name_2002' + '.npy')\n",
    "ID_names_summaries = []\n",
    "for item in ID_names_summaries2:\n",
    "    if item != 'd082a' and item != 'd094c' and item != 'd099e' and item != 'd105g':\n",
    "        ID_names_summaries.append(item)\n",
    "\n",
    "        \n",
    "ID_names_summaries.sort()\n",
    "\n",
    "# ID names of gold standard documents DUC 2002\n",
    "ID_names_gold_std = ID_names_summaries\n",
    "\n",
    "# DUC year\n",
    "year = '2002'\n",
    "\n",
    "# ID name of summaries and gold standard documents\n",
    "name_summmary = '_englishSyssym1'\n",
    "name_goold_std = '_englishReference1'\n",
    "\n",
    "# ID gold standard\n",
    "ID_gold_std = ['A', 'B']\n",
    "\n",
    "gs = 'all_golds'\n",
    "\n",
    "# principal path for gold standard documents\n",
    "#pr_gold_std_path = '/Users/flintlock/Desktop/model_summaries{}_2002_unsup'.format(ID_gold_std)\n",
    "\n",
    "# secondary path for gold standard documents\n",
    "sc_gold_std_path = current_dir + '/model_summariesA_2004_sup'\n",
    "\n",
    "# secondary path for summaries\n",
    "sc_summaries_path = current_dir + '/DUC2004_for_labelling'\n",
    "\n",
    "# ROUGE path for gold standard documents\n",
    "rg_gold_std_path = rg_path + '/model_summariesA_2004_sup'\n",
    "\n",
    "# ROUGE path for summaries\n",
    "rg_summaries_path = rg_path + '/system_summaries_2004_labelling' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_evaluate(max_d_list, diff_bits_list, max_d_sent_list, size_summary, texts_list):\n",
    "    \n",
    "    %run Summarize_Texts.ipynb\n",
    "\n",
    "    Resume_Textos(max_d_list = max_d_list,  # Entre [0,1] -> 0.9 buen valor\n",
    "                  diff_bits_list = diff_bits_list,  # Entre [0, 5000] -> 2500 buen valor\n",
    "                  #max_d_sent_list = [4000, 4100, 4250, 4300, 4500, 4600, 4750], # Entre [0, 5000] -> 4500 buen valor\n",
    "                  max_d_sent_list = max_d_sent_list,\n",
    "                  size_summary = size_summary,   # Tama√±o del texto [100, 200, 400])\n",
    "                  texts_list = texts_list)  # Por default 57 (todos)\n",
    "    \n",
    "    eva = ROUGE_measures_summaries(ID_gold_std, ID_names_summaries, ID_names_gold_std, name_goold_std, sc_gold_std_path, name_summmary, rg_gold_std_path, sc_summaries_path, rg_summaries_path, rg_path, ps, pm, year, folder_results, folder_experiment)    \n",
    "\n",
    "    res=[1-x for x in eva]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = resume_textos_y_evalua([0.985], [1750], [4375], 200, 57)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
