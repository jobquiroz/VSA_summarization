{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook contiene la funciÃ³n principal para resumir un conjunto de documentos usando un rango dado de parametros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For clustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "# Para iterar sobre archivos\n",
    "import os\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg') #_sm, _md, _lg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling HDComputing_basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run HDComputing_basics.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing texts\n",
    "\n",
    "### Functions for language processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_to_list (filename):\n",
    "    # Reading file and deleting blank spaces\n",
    "    Text = []\n",
    "    with open(filename, \"r\" , encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "    for l in lines:\n",
    "        if l[2:-2] != '':\n",
    "            Text.append(l[2:-2])\n",
    "    return Text\n",
    "\n",
    "def modify_sent (sentence, list_starts_ends): # String, ['label_str', starts_int, end_int]\n",
    "    \"It substitute labels for entitites in sentences, e.g.  I was in Mexico -> I was in GPE0\"\n",
    "    \n",
    "    def change_order (List_index):\n",
    "        \"Change representation\"\n",
    "        new_list = [[0, List_index[0][1], List_index[0][0]]]\n",
    "        for i in range(len(List_index) - 1):\n",
    "            new_list.append([List_index[i][2], List_index[i+1][1], List_index[i+1][0]])\n",
    "        new_list.append([List_index[-1][2], None, None])  #NO SE DEBE HACER CASO \n",
    "        return new_list\n",
    "    \n",
    "    if len(list_starts_ends) == 0:\n",
    "        return sentence\n",
    "    else:\n",
    "        new_repr = change_order(list_starts_ends)\n",
    "        new_sentence = ''\n",
    "        for x in new_repr[:-1]:\n",
    "            new_sentence += sentence[x[0]: x[1]] + x[2]\n",
    "\n",
    "        new_sentence += sentence[new_repr[-1][0]:]\n",
    "        return new_sentence\n",
    "\n",
    "def entity_text_recognizer (Text):\n",
    "    \"\"\" Given a list of sentences (text) it recognizes entities and returns the same text with \n",
    "    entity labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # To return\n",
    "    Text_mod = []\n",
    "    Text_copy = [] \n",
    "    Dict_word_2_entlabel = {}  # Mexico (string) -> GPE#\n",
    "    Dict_entlabel_2_word = {}  # GPE# -> Mexico (span)\n",
    "    \n",
    "    # Entity counter\n",
    "    Dict_ent_count = {} # Diccionario contador de entidades\n",
    "    \n",
    "    for sentence in Text:\n",
    "        # Processing\n",
    "        doc = nlp(sentence)\n",
    "        \n",
    "        # Removing noise sentences\n",
    "        if len(doc) > 6:   \n",
    "            # Iterate over entities\n",
    "            chars_to_change = []\n",
    "            for ent in doc.ents:\n",
    "                # This label exists!\n",
    "                try:\n",
    "                    new_label = Dict_word_2_entlabel[str(ent)]\n",
    "\n",
    "                except: # A new label...\n",
    "                    try:\n",
    "                        Dict_ent_count[ent.label_] += 1\n",
    "                    except:\n",
    "                        Dict_ent_count[ent.label_] = 1\n",
    "\n",
    "                    new_label = ent.label_ + str(Dict_ent_count[ent.label_])\n",
    "                    # Two way dictionary\n",
    "                    Dict_entlabel_2_word[new_label] = ent\n",
    "                    Dict_word_2_entlabel[str(ent)] = new_label\n",
    "\n",
    "                chars_to_change.append([ new_label, ent.start_char, ent.end_char])\n",
    "\n",
    "            # Changing text\n",
    "            Text_mod.append(modify_sent(sentence, chars_to_change))\n",
    "            Text_copy.append(sentence)\n",
    "\n",
    "    \n",
    "    # Entities array\n",
    "    Entities = list(Dict_entlabel_2_word.keys())\n",
    "    Entities_array = np.array([np.array([x]) for x in Entities])\n",
    "    \n",
    "    return Text_mod, Text_copy, Dict_word_2_entlabel, Dict_entlabel_2_word, Entities_array\n",
    "\n",
    "def Text_to_POS_arrays (Text_mod):\n",
    "    \"This functions creates a tokens array for each POS tag\"\n",
    "    # Dictionary of POS\n",
    "    Dict_word_pos = {'ADJ': set(), 'ADV': set(), 'NOUN': set(), 'VERB': set() }\n",
    "\n",
    "    for sentence in Text_mod:\n",
    "        # Processing\n",
    "        doc = nlp(sentence)\n",
    "        # Iterate over each token\n",
    "        for token in doc:\n",
    "            if not(token.is_stop) and token.pos_ != 'PUNCT' and token.pos_ != 'SYM' \\\n",
    "               and token.text != '-' and token.text != '_' and token.text != '.' and token.text != \"'\" \\\n",
    "               and (token.text not in Dict_entlabel_2_word.keys()): # Este ultimo es para que no sea entidad\n",
    "                    # Setting dict key\n",
    "                    if token.pos_ == 'PROPN':\n",
    "                        keyy = 'NOUN'\n",
    "                    elif token.pos_ not in ['ADJ', 'ADV', 'NOUN', 'VERB']:\n",
    "                        continue # Next word... we only consider these 4 classes\n",
    "                    else:\n",
    "                        keyy = token.pos_\n",
    "\n",
    "                    # No lemmatization is applied\n",
    "                    Dict_word_pos[keyy].add(token.text)\n",
    "    \n",
    "    # From dictionaries to arrays\n",
    "    # Adjectives\n",
    "    Adj_array = np.array([np.array([ nlp(x)[0] ]) for x in Dict_word_pos['ADJ']])\n",
    "    # Adverbs\n",
    "    Adv_array = np.array([np.array([ nlp(x)[0] ]) for x in Dict_word_pos['ADV']])\n",
    "    # Nouns\n",
    "    Noun_array = np.array([np.array([ nlp(x)[0] ]) for x in Dict_word_pos['NOUN']])\n",
    "    # Verbs\n",
    "    Verb_array = np.array([np.array([ nlp(x)[0] ]) for x in Dict_word_pos['VERB']])\n",
    "    \n",
    "    return Adj_array, Adv_array, Noun_array, Verb_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_spacy_ent(label_ent1, label_ent2):\n",
    "    \"Distance between two entity words\"\n",
    "    global Dict_entlabel_2_word\n",
    "    entity1 = Dict_entlabel_2_word[label_ent1[0]]\n",
    "    entity2 = Dict_entlabel_2_word[label_ent2[0]]\n",
    "    if entity1.has_vector and entity2.has_vector:\n",
    "        dist = 1 - entity1.similarity(entity2)\n",
    "        if dist < 0:\n",
    "            return 0\n",
    "        elif dist > 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return dist\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def dist_spacy(lem1, lem2):\n",
    "    \"Distance between words (except entities)\"\n",
    "    if lem1[0].has_vector and lem2[0].has_vector:\n",
    "        dist = 1 - lem1[0].similarity(lem2[0])\n",
    "        if dist < 0:\n",
    "            return 0\n",
    "        elif dist > 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return dist\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def plot_dendrogram(distance_array, max_distance):\n",
    "    \"Plot a dendrogram\"\n",
    "    plt.figure(figsize=(25,10))\n",
    "    plt.title(\"Hierachical Clustering Dendrogram\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Average distance\")\n",
    "    dn = dendrogram(distance_array,\n",
    "               leaf_rotation=90.,\n",
    "               leaf_font_size=9.,\n",
    "               color_threshold = max_distance);\n",
    "    plt.axhline(max_distance, c='k')\n",
    "\n",
    "\n",
    "def set_cluster_list(distance_array, words_array, max_distance ):\n",
    "    \"Dividing clusters by threshold distance\"\n",
    "    clusters = fcluster(distance_array, max_distance, criterion='distance')\n",
    "    uniques = np.unique(clusters)\n",
    "    # New list with lists for each cluster\n",
    "    cluster_list = []\n",
    "    for i in uniques: # For each numbered cluster\n",
    "        cluster_list.append([words_array[j] for j in range(len(clusters)) if clusters[j] == i])\n",
    "    return sorted(cluster_list, key = len, reverse = True)\n",
    "\n",
    "def medoid_token (cluster):  # Recibe una arreglo de strings: [['lemm_0'], ['lemm_1'], ..., ['lemm_n']]\n",
    "    \"Index of the centroid element\"\n",
    "    if len(cluster) < 3:\n",
    "        return 0 # Da igual\n",
    "    else:\n",
    "        # Cambiar formato -> array[ arrays...]\n",
    "        #clus = np.array(cluster)\n",
    "        clus = cluster #?!\n",
    "        \n",
    "        # Medir matriz de distancia (completa)\n",
    "        dist_mat = squareform(pdist(clus, metric = dist_spacy))\n",
    "        # Regresar minimo\n",
    "        return np.argmin(dist_mat.sum(axis = 0))\n",
    "\n",
    "def medoid_entity (cluster):  # Recibe una arreglo de etiquetas de entidades (strings)\n",
    "    \"Index of the centroid element\"\n",
    "    if len(cluster) < 3:\n",
    "        return 0 # Da igual\n",
    "    else:\n",
    "        # Format\n",
    "        #cluster = np.array([np.array([x]) for x in cluster])\n",
    "        # Measuring distance\n",
    "        dist_mat = squareform(pdist(cluster, metric = dist_spacy_ent))\n",
    "        # Returns min\n",
    "        return np.argmin(dist_mat.sum(axis = 0))\n",
    "\n",
    "def medoid_sentence (cluster):  # Recibe una lista de oraciones\n",
    "    \"Finds medoid sentence\"\n",
    "    if len(cluster) < 3:\n",
    "        return 0 #PROBABLEMENTE DEBA DESCARTAR ESTOS CASOS\n",
    "    else:\n",
    "        # Midiendo distancias manual... \n",
    "        X = []\n",
    "        for i in range(len(cluster) - 1):\n",
    "            for j in range(i+1, len(cluster)):\n",
    "                X.append( Dict[cluster[i]].dist(Dict[cluster[j]]) )\n",
    "                #print(\"sentencia {} vs sentencia {} = {}\".format(i, j, X[-1]))\n",
    "                \n",
    "        X_arr = squareform(np.array(X))\n",
    "        \n",
    "        # Regresar minimo\n",
    "        return np.argmin(X_arr.sum(axis = 0))\n",
    "    \n",
    "def distancias_POS_arrays (Adj_array, Adv_array, Noun_array, Verb_array, Entities_array):\n",
    "    # Adj\n",
    "    X_adj = pdist(Adj_array, metric = dist_spacy)\n",
    "    distance_adj = linkage(X_adj, 'complete')\n",
    "\n",
    "    # Adv\n",
    "    X_adv = pdist(Adv_array, metric = dist_spacy)\n",
    "    distance_adv = linkage(X_adv, 'complete')\n",
    "\n",
    "    # Nouns\n",
    "    X_nn = pdist(Noun_array, metric = dist_spacy)\n",
    "    distance_nn = linkage(X_nn, 'complete')\n",
    "\n",
    "    # Verbs\n",
    "    X_vrb = pdist(Verb_array, metric = dist_spacy)\n",
    "    distance_vrb = linkage(X_vrb, 'complete')\n",
    "\n",
    "    # Entities\n",
    "    X_ent = pdist(Entities_array, metric = dist_spacy_ent)\n",
    "    distance_ent = linkage(X_ent, 'complete')\n",
    "    \n",
    "    return distance_adj, distance_adv, distance_nn, distance_vrb, distance_ent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperdimensional space mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Assign_HDvectors (cluster, medoids, POS, diff_bits):\n",
    "    \"Given a medoids and clusters arrays it assigns HD vectors to each item\"\n",
    "    def name_item (word_in_array, pos):\n",
    "        \"Creates string label:  text + . + pos_\"\n",
    "        return str(word_in_array[0]) + '.'+ pos\n",
    "    \n",
    "    for i in range(len(cluster)):\n",
    "        # indice del medoide\n",
    "        medoid_ix = medoids[i]\n",
    "        #Primero se asigna un vector aleatorio al medoide\n",
    "        Dict_HD[ name_item( cluster[i][medoid_ix], POS ) ] = HDvector(N, name_item( cluster[i][medoid_ix], POS) ) \n",
    "\n",
    "        seed_vec = Dict_HD[name_item( cluster[i][medoid_ix], POS )].getVec()\n",
    "        # Se asigna el resto de vectores (contaminados)\n",
    "        for j in range(len(cluster[i])):\n",
    "            if j != medoid_ix:\n",
    "                Dict_HD[name_item( cluster[i][j], POS )] = HDvector( contamina_vec(seed_vec, diff_bits) , name_item( cluster[i][j], POS ))   \n",
    "                \n",
    "# FunciÃ³n de distancia\n",
    "def distancia_HD (HDvector_1, HDvector_2):\n",
    "    obj1 = HDvector_1[0]\n",
    "    obj2 = HDvector_2[0]\n",
    "    return obj1.dist(obj2)\n",
    "\n",
    "\n",
    "def Encode_sentences (Text_mod):\n",
    "    \"Dado el texto modificado Y ya teniendo generados los vectores HD de cada palabra, \\\n",
    "    esta funciÃ³n regresa un arreglo con los vectores de cada oraciÃ³n\"\n",
    "    Sentences_done = set()\n",
    "    Vectors_sentence = []\n",
    "\n",
    "    for s in Text_mod:\n",
    "        if s not in Sentences_done and s not in Dict_word_2_entlabel.keys(): \n",
    "          #  Non-repeated          &    not 1-word sentences...\n",
    "    \n",
    "            Sentences_done.add(s)  # Add to done      \n",
    "            vec_words = []         # Stores vectors for latter addition\n",
    "            sentence = nlp(s)\n",
    "            for token in sentence:\n",
    "                if not(token.is_stop) and (token.pos_ != 'PUNCT') and (token.pos_ != 'SYM') \\\n",
    "                   and token.text != '-' and token.text != '_' and token.text != '.' and token.text != \"'\":\n",
    "\n",
    "                    try: # Entites case\n",
    "                        vec_words.append(Dict_HD[token.text + '.ENT'])\n",
    "                    except KeyError:\n",
    "\n",
    "                        # No lemmatization... \n",
    "                        word = token.text  \n",
    "\n",
    "                        if token.pos_ == 'PROPN':\n",
    "                            keyy = 'NOUN'\n",
    "                        elif token.pos_ not in ['ADJ', 'ADV', 'NOUN', 'VERB']:\n",
    "                            continue\n",
    "                        else:\n",
    "                            keyy = token.pos_\n",
    "\n",
    "                        vec_words.append(Dict_HD[word + '.' + keyy])\n",
    "\n",
    "            # Adding sentence vector to list\n",
    "            if len(vec_words) > 0:\n",
    "                Vectors_sentence.append( HDvector(ADD(vec_words).getVec(), s) )\n",
    "    \n",
    "    # Returns as array\n",
    "    return np.array([np.array([x]) for x in Vectors_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_sentence (sent_mod, Tex_mod, Text_copy):\n",
    "    \"Regresa el valor original de las etiquetas de entidad\"\n",
    "    #print('len(Text_mod):', len(Text_mod))\n",
    "    #print('len(Text_copy):', len(Text_copy)) \n",
    "    for i in range(len(Text_mod)):\n",
    "        if sent_mod == Text_mod[i]:\n",
    "            return Text_copy[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_text(resumen):\n",
    "    \"Counting words in summary\"\n",
    "    lines = resumen.replace('\\n', ' ').replace(',', '').replace('.', '').replace(':', '').replace(';', '').replace('`','')\n",
    "    doc = nlp(lines)\n",
    "    i = 1\n",
    "    for x in doc:\n",
    "        if not x.is_punct and x.text not in [\" \", \"\\'s\", \"n't\", \"$\"]:\n",
    "            i += 1\n",
    "    return i - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Summarize_texts(max_d_list,   \n",
    "                  diff_bits_list,\n",
    "                  max_d_sent_list,\n",
    "                  size_summary = 200, # TamaÃ±o del resumen\n",
    "                  texts_list = 57,\n",
    "                  Directory = 'Texts/', \n",
    "                  Precalc_Directory = 'Precalc_distances/',\n",
    "                  output_dir = 'Summaries/'):\n",
    "    \n",
    "    # Dimensionality\n",
    "    N = 10000\n",
    "    # Intialization\n",
    "    init(N)\n",
    "    \n",
    "    # Globales variables\n",
    "    global Dict_entlabel_2_word, Text_mod, Text_copy, Dict_word_2_entlabel, Dict_entlabel_2_word, Entities_array\n",
    "    global Adj_array, Adv_array, Noun_array, Verb_array\n",
    "\n",
    "    \n",
    "    # Loop over all documents\n",
    "    for filename in os.listdir(Directory)[20:texts_list]: \n",
    "        \n",
    "        # Making dir\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "                    \n",
    "        print(filename)\n",
    "        dict_HD = {}\n",
    "        # Reading file\n",
    "        Text = read_file_to_list(Directory + filename)\n",
    "\n",
    "        # Entity recognition\n",
    "        Text_mod, Text_copy, Dict_word_2_entlabel, Dict_entlabel_2_word, Entities_array = entity_text_recognizer(Text)\n",
    "\n",
    "        # POS classification\n",
    "        Adj_array, Adv_array, Noun_array, Verb_array = Text_to_POS_arrays(Text_mod)\n",
    "\n",
    "        # To save time we use precalc distances,\n",
    "        # the first time it is used it generates the dirs\n",
    "        if not os.path.exists(Precalc_Directory):\n",
    "            os.mkdir(Precalc_Directory)\n",
    "        if filename[:-4] + '.npy' in os.listdir(Precalc_Directory):\n",
    "         #   print('Precalc distances')\n",
    "            pre_computed_arrays = np.load(Precalc_Directory + filename[:-4] + '.npy', allow_pickle = True)\n",
    "            # Assigning arrays\n",
    "            distance_adj = pre_computed_arrays[0]\n",
    "            distance_adv = pre_computed_arrays[1]\n",
    "            distance_nn = pre_computed_arrays[2]\n",
    "            distance_vrb = pre_computed_arrays[3]\n",
    "            distance_ent = pre_computed_arrays[4]\n",
    "        else:\n",
    "           # print('No precalc distances')\n",
    "            # Measuring distance\n",
    "            distance_adj, distance_adv, distance_nn, distance_vrb, distance_ent = distancias_POS_arrays(Adj_array, \n",
    "                                                                                                    Adv_array, \n",
    "                                                                                                    Noun_array, \n",
    "                                                                                                    Verb_array,\n",
    "                                                                                                    Entities_array)\n",
    "\n",
    "            master_array = np.array([distance_adj, distance_adv, distance_nn, distance_vrb, distance_ent])\n",
    "            np.save(Precalc_Directory + filename[:-4] + '.npy', master_array)\n",
    "\n",
    "\n",
    "        for max_d in max_d_list:\n",
    "                \n",
    "            # Adjectives:\n",
    "            Clusters_adj = set_cluster_list(distance_adj, Adj_array, max_d )\n",
    "            medoids_adj = [medoid_token(x) for x in Clusters_adj]\n",
    "\n",
    "            # Adverbs\n",
    "            Clusters_adv = set_cluster_list(distance_adv, Adv_array, max_d )\n",
    "            medoids_adv = [medoid_token(x) for x in Clusters_adv]\n",
    "\n",
    "            # Nouns\n",
    "            Clusters_nn = set_cluster_list(distance_nn, Noun_array, max_d )\n",
    "            medoids_nn = [medoid_token(x) for x in Clusters_nn]\n",
    "\n",
    "            # Verb\n",
    "            Clusters_vrb = set_cluster_list(distance_vrb, Verb_array, max_d )\n",
    "            medoids_vrb = [medoid_token(x) for x in Clusters_vrb]\n",
    "\n",
    "            # Entities\n",
    "            Clusters_ent = set_cluster_list(distance_ent, Entities_array , max_d )\n",
    "            medoids_ent = [medoid_entity(x) for x in Clusters_ent]\n",
    "\n",
    "\n",
    "            for diff_bits in diff_bits_list:\n",
    "                # Initializing HD memory\n",
    "                init(N)\n",
    "                # Adjectives\n",
    "                Assign_HDvectors(Clusters_adj, medoids_adj, 'ADJ', diff_bits)\n",
    "                # Adverbs\n",
    "                Assign_HDvectors(Clusters_adv, medoids_adv, 'ADV', diff_bits)\n",
    "                # Nouns\n",
    "                Assign_HDvectors(Clusters_nn, medoids_nn, 'NOUN', diff_bits)\n",
    "                # Verbs\n",
    "                Assign_HDvectors(Clusters_vrb, medoids_vrb, 'VERB', diff_bits)\n",
    "                # Entities\n",
    "                Assign_HDvectors(Clusters_ent, medoids_ent, 'ENT', diff_bits)\n",
    "\n",
    "                # Sentece encoding\n",
    "                Vectors_sentence_array = Encode_sentences(Text_mod)\n",
    "                # Clustering sentences\n",
    "                X_sents = pdist(Vectors_sentence_array, metric = distancia_HD)\n",
    "                distance_sent = linkage(X_sents, 'complete')\n",
    "                \n",
    "                sentt = [x[0].getLabelID()[0][0] for x in Vectors_sentence_array]  \n",
    "\n",
    "                for max_d_sent in max_d_sent_list:\n",
    "                    # Directory name for results\n",
    "                    dirName = output_dir + str(int(N/1000)) + 'k_' + str(max_d) + '_' + str(diff_bits) + '_' + str(max_d_sent)\n",
    "                    \n",
    "                    # Making dir\n",
    "#                    if not os.path.exists(dirName):\n",
    "#                        os.mkdir(dirName)\n",
    "#                    else:\n",
    "#                        if os.path.exists(dirName + '/' + filename[:-4] + '_englishSyssym1.txt'):\n",
    "                         #   print('Ya existe archivo de resumenes, siguiente...')\n",
    "#                            continue\n",
    "                    \n",
    "                    clusters = fcluster(distance_sent, max_d_sent, criterion='distance')\n",
    "                    uniques = np.unique(clusters)\n",
    "            \n",
    "                    # New list for clusters\n",
    "                    Clusters_sentences = [[] for i in range(len(uniques))]\n",
    "                    \n",
    "                    # Sorting clusters\n",
    "                    for i in range(len(clusters)): \n",
    "                        Clusters_sentences[clusters[i] - 1].append(sentt[i])\n",
    "                    \n",
    "                    Clusters_sentences = sorted(Clusters_sentences, key = len, reverse = True)\n",
    "                    # Centroid\n",
    "                    medoids = [medoid_sentence(x) for x in Clusters_sentences]\n",
    "\n",
    "                    # Creating summary\n",
    "                    Resumen = ''\n",
    "                    i_medoids = -1 # For indexing cluster\n",
    "                    for s in Clusters_sentences:\n",
    "                        i_medoids += 1\n",
    "                        # sentence to be added...\n",
    "                        sentences_to_prob_add = original_sentence(s[medoids[i_medoids]], Text_mod, Text_copy) + '\\n'\n",
    "                        if count_words_text(Resumen + sentences_to_prob_add) <= size_summary + 10: #210...\n",
    "                            Resumen += sentences_to_prob_add  # Adding sentence to summary\n",
    "\n",
    "                        # Size limit\n",
    "                        elif count_words_text(Resumen) >= size_summary - 10: # 190...\n",
    "                            break\n",
    "\n",
    "                    # Final summary file\n",
    "                    with open(output_dir + filename[:-4] + dirName[17:] +  '_englishSyssym1.txt', 'w') as f:\n",
    "                        f.write(Resumen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
